{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that can get 100 posts from a provided subreddit using pushshift API.\n",
    "# Please note, 100 posts were the maximum number of post can be obained form reddit each time at the time when the function was created.\n",
    "def get100_from_reddit(subreddit = 'StarWars', before = None):\n",
    "#      input:  subreddit -> a string for the subreddit name\n",
    "#               before - > the time stamp tells when the post was created, in the format of unix time (https://en.wikipedia.org/wiki/Unix_time)\n",
    "#                            If it is given, this function only tries to get the post created before this time\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    params = {\n",
    "    'subreddit' : subreddit,\n",
    "    'size' : 100,\n",
    "    'before' : before\n",
    "    }\n",
    "    res = requests.get(url,params)\n",
    "    if res.status_code == 200:\n",
    "        data = res.json()\n",
    "        posts = data['data']\n",
    "    else:\n",
    "        print(f\"request status with error code: {res.status_code}\")   # print out the the error information\n",
    "    return pd.DataFrame(posts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get100_from_reddit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1597808887\n",
       "1     1597808693\n",
       "2     1597808325\n",
       "3     1597808200\n",
       "4     1597808014\n",
       "         ...    \n",
       "95    1597781950\n",
       "96    1597781870\n",
       "97    1597781475\n",
       "98    1597781320\n",
       "99    1597781030\n",
       "Name: created_utc, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['created_utc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that get the target amount of posts from the provided subreddit from reddit.com\n",
    "def get_from_reddit(subreddit = 'StarWars', num_rows = 500):\n",
    "#     input: subreddit -> a string for the subreddit name\n",
    "#              num_rows -> The number of posts needed\n",
    "    \n",
    "#     No restrictions on the first 100 posts\n",
    "    before = None \n",
    "    \n",
    "#     The dataframe for the obtained posts\n",
    "    df = get100_from_reddit(subreddit = subreddit)\n",
    "    \n",
    "    for i in range(200,num_rows+100,100):\n",
    "#         Put time restriction on the posts by the oldest post which had been obained already\n",
    "        before = df.iloc[df.shape[0]-1,:]['created_utc']\n",
    "#     attached the newly obtained to the end of the dataframe which had all the posts we have already got\n",
    "        df = pd.concat([df,get100_from_reddit(subreddit = subreddit,before=before)],axis = 0)\n",
    "        \n",
    "#         Printing out a message after getting every 1000 posts\n",
    "        if i%1000 == 0:\n",
    "            print(f\"Have got {i} rows of data \")\n",
    "#         Take 1 second break after getting 100 post each time\n",
    "        time.sleep(1)\n",
    "#     Return a dataframe which had all posts\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have got 1000 rows of data \n",
      "Have got 2000 rows of data \n",
      "Have got 3000 rows of data \n",
      "Have got 4000 rows of data \n",
      "Have got 5000 rows of data \n",
      "Have got 6000 rows of data \n",
      "Have got 7000 rows of data \n",
      "Have got 8000 rows of data \n",
      "Have got 9000 rows of data \n",
      "Have got 10000 rows of data \n",
      "Have got 11000 rows of data \n",
      "Have got 12000 rows of data \n",
      "Have got 13000 rows of data \n",
      "Have got 14000 rows of data \n",
      "Have got 15000 rows of data \n",
      "Have got 16000 rows of data \n",
      "Have got 17000 rows of data \n",
      "Have got 18000 rows of data \n",
      "Have got 19000 rows of data \n",
      "Have got 20000 rows of data \n"
     ]
    }
   ],
   "source": [
    "# Obtain 20,000K posts from 'StarWars'(default subreddit) subreddit\n",
    "df = get_from_reddit(num_rows = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have got 1000 rows of data \n",
      "Have got 2000 rows of data \n",
      "Have got 3000 rows of data \n",
      "Have got 4000 rows of data \n",
      "Have got 5000 rows of data \n",
      "Have got 6000 rows of data \n",
      "Have got 7000 rows of data \n",
      "Have got 8000 rows of data \n",
      "Have got 9000 rows of data \n",
      "Have got 10000 rows of data \n",
      "Have got 11000 rows of data \n",
      "Have got 12000 rows of data \n",
      "Have got 13000 rows of data \n",
      "Have got 14000 rows of data \n",
      "Have got 15000 rows of data \n",
      "Have got 16000 rows of data \n",
      "Have got 17000 rows of data \n",
      "Have got 18000 rows of data \n",
      "Have got 19000 rows of data \n",
      "Have got 20000 rows of data \n"
     ]
    }
   ],
   "source": [
    "# Obtain 20,000K posts from 'marvelstudios' subreddit\n",
    "df_marvel = get_from_reddit(subreddit='marvelstudios', num_rows = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the obtained datasets\n",
    "df.to_csv('./data/starwar_0820.csv')\n",
    "df_marvel.to_csv('./data/marvel_0820.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
